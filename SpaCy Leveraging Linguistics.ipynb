{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesud\\Anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_lg' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy # for visualization\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redacting Names with Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text with SpaCy. This runs the entire NLP pipeline\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'doc' now contains a parsed version of text. We can use it to do anything we want!. For example, this will print out all the named entities that were detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomfrey (PERSON)\n",
      "Pepperup (PERSON)\n",
      "several hours (TIME)\n",
      "Ginny Weasley (PERSON)\n",
      "Percy (PERSON)\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pomfrey, Pepperup, several hours, Ginny Weasley, Percy)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 'PERSON')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.label, entity.label_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In spaCy, all human readable labels etc can also be explained using the simple spacy.explain(label) syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentence = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            redacted_sentence.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentence.append(token.string)\n",
    "    return \"\".join(redacted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam [REDACTED], the nurse, was kept busy by a sudden spate of colds among the staff and students. Her [REDACTED]potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. [REDACTED][REDACTED], who had been looking pale, was bullied into taking some by [REDACTED].'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redact_names(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Types"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lets look at some examples of above in real world sentences, we will also use the spacy.explain() on all entities to build a quick mental model of how these things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_text_entities(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(f'{ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla, Label: ORG, Companies, agencies, institutions, etc.\n",
      "20%, Label: PERCENT, Percentage, including \"%\"\n",
      "the months, Label: DATE, Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Tesla has gained 20% market share in the months since')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taj Mahal, Label: ORG, Companies, agencies, institutions, etc.\n",
      "Mughal, Label: NORP, Nationalities or religious or political groups\n",
      "Shah Jahan, Label: PERSON, People, including fictional\n",
      "Yamuna, Label: ORG, Companies, agencies, institutions, etc.\n",
      "Agra, Label: GPE, Countries, cities, states\n",
      "India, Label: GPE, Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Taj Mahal built by Mughal Emperor Shah Jahan stands tall on the banks of Yamuna in modern day Agra, India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashoka, Label: ORG, Companies, agencies, institutions, etc.\n",
      "Indian, Label: NORP, Nationalities or religious or political groups\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Ashoka was a great Indian king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashoka University, Label: ORG, Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('The Ashoka University sponsors the Young India Fellowship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'Bansoori is an Indian classical instrument. Tom plays Bansoori and Guitar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(example_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need noun chunks. Noun chunks are noun phrases - not a single word, but a short phrase which describes the noun.\n",
    "For example, \"the blue skies\" or \"the worldâ€™s largest conglomerate\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To get the noun chunks in a document, simply iterate over doc.noun_chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 Bansoori\n",
      "sentence1 an Indian classical instrument\n",
      "sentence2 Tom\n",
      "sentence2 Bansoori\n",
      "sentence2 Guitar\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(doc.sents):\n",
    "    for noun in sentence.noun_chunks:\n",
    "        print(f'sentence{idx+1}', noun)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Our example text has two sentences, we can pull out noun phrase chunks from each sentence. We pull out noun phrases instead of single words. This means, we are able to pull out 'an Indian classical instrument' as one noun. This is quite useful as we will see in a moment."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, lets take a quick look at all parts-of-speech tags in our example text. We will use the verbs and adjectives to write some simple question generating logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori PROPN NNP\n",
      "is AUX VBZ\n",
      "an DET DT\n",
      "Indian ADJ JJ\n",
      "classical ADJ JJ\n",
      "instrument NOUN NN\n",
      ". PUNCT .\n",
      "Tom PROPN NNP\n",
      "plays VERB VBZ\n",
      "Bansoori PROPN NNP\n",
      "and CCONJ CC\n",
      "Guitar PROPN NNP\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Ruleset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Creating a Ruleset\n",
    "Quite often when using linguistics, you will be writing custom rules. Here is one data structure suggestion to help you store these rules: list of dictionaries. Each dictionary in turn can have elements ranging from simple string lists to lists to strings. Avoid nesting a list of dictionaries inside a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = [\n",
    "    {\n",
    "        'id': 1, \n",
    "        'req_tags': ['NNP', 'VBZ', 'NN'],\n",
    "    }, \n",
    "    {\n",
    "        'id': 2, \n",
    "        'req_tags': ['NNP', 'VBZ'],\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, I have written two rules. Each rule is simply a collection of part-of-speech tags stored under the 'req_tags' key. Each rule comprises of all the tags that I will look for in a particular sentence.\n",
    "\n",
    "Depending on 'id', I will use a hard coded question template to generate my questions. In practice, you can and should move the question template to your ruleset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'req_tags': ['NNP', 'VBZ', 'NN']}, {'id': 2, 'req_tags': ['NNP', 'VBZ']}]\n"
     ]
    }
   ],
   "source": [
    "print(ruleset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, I need a function to pull out all tokens which match a particular tag. We do this by simply iterating over the entire list of and matching each token against the target tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(doc, tag):\n",
    "    return [tok for tok in doc if tok.tag_ == tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_ques(sent:str)->str:\n",
    "    \"\"\"\n",
    "    Return a question string corresponding to a sentence string using a set of pre-written rules\n",
    "    \"\"\"\n",
    "    doc = nlp(sent)\n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    for idx, rule in enumerate(ruleset):\n",
    "        if rule['id'] == 1:\n",
    "            if all(key in pos_tags for key in rule['req_tags']): \n",
    "                print(f\"Rule id {rule['id']} matched for sentence: {sent}\")\n",
    "                NNP = get_pos_tag(doc, \"NNP\")\n",
    "                NNP = str(NNP[0])\n",
    "                VBZ = get_pos_tag(doc, \"VBZ\")\n",
    "                VBZ = str(VBZ[0])\n",
    "                ques = f'What {VBZ} {NNP}?'\n",
    "                return(ques)\n",
    "        if rule['id'] == 2:\n",
    "            if all(key in pos_tags for key in rule['req_tags']): #'NNP', 'VBZ' in sentence.\n",
    "                print(f\"Rule id {rule['id']} matched for sentence: {sent}\")\n",
    "                NNP = get_pos_tag(doc, \"NNP\")\n",
    "                NNP = str(NNP[0])\n",
    "                VBZ = get_pos_tag(doc, \"VBZ\")\n",
    "                VBZ = str(VBZ[0].lemma_)\n",
    "                ques = f'What does {NNP} {VBZ}?'\n",
    "                return(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule id 1 matched for sentence: Bansoori is an Indian classical instrument.\n",
      "The generated quietion is : What is Bansoori?\n",
      "Rule id 2 matched for sentence: Tom plays Bansoori and Guitar.\n",
      "The generated quietion is : What does Tom play?\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(f\"The generated quietion is : {sent_to_ques(str(sent))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3] *",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
