{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpart Practice",
      "provenance": [],
      "authorship_tag": "ABX9TyPJWre8Dxs0RHzEIwChUiLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jesudhas/Machine-learning/blob/master/PySpart_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuBMK0bAaYJG"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-C8OYVxajlh"
      },
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FRi_ndamEp"
      },
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9StwfKNUau-E"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8JW1qp8azP9"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVttwn7na4gd",
        "outputId": "8a0eb9e5-04ea-4d32-c77a-0a1e3f1b3103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(10, False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RqgTwMka8r9",
        "outputId": "6671fe35-aa57-4a3a-d75a-e4e99b0fe092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[6,8,13,16],[...|\n",
            "|  0.0|(20,[0,2,7,13,15,...|\n",
            "|  1.0|(20,[3,4,6,11,19]...|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL8lfVksdHWN",
        "outputId": "28e5753b-fbbd-4bc1-d16d-b30f8bd77400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Create some dummy feature data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )\n",
        "features_df.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwwo1IhQdppO",
        "outputId": "adba7240-3546-42f9-95e2-02fd280e9d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "features_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")\n",
        "smodel = features_scaler.fit(features_df)\n",
        "sfeatures_df = smodel.transform(features_df)\n",
        "sfeatures_df.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|           (3,[],[])|\n",
            "|  2|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
            "|  3|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vazruOIgjPa"
      },
      "source": [
        "Standardize Numeric Data\n",
        "StandardScaler is another well-known class written with machine learning libraries. It normalizes the data between -1 and 1 and converts the data into bell-shaped data. You can demean the data and scale to some variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsnNTKOem0W"
      },
      "source": [
        "from pyspark.ml.feature import  StandardScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAxYBUf9gDPN"
      },
      "source": [
        "# Create the dummy data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AZQs1xXgH09",
        "outputId": "dba1f0a8-0fd3-4909-b8b5-7e4d3d8a0dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Apply the StandardScaler model\n",
        "features_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd=True, withMean=True)\n",
        "stmodel = features_stand_scaler.fit(features_df)\n",
        "stand_sfeatures_df = stmodel.transform(features_df)\n",
        "stand_sfeatures_df.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
            "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
            "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZujG4NgnEC"
      },
      "source": [
        "Bucketize Numeric Data\n",
        "The real data sets come with various ranges and sometimes it is advisable to transform the data into well-defined buckets before plugging into machine learning algorithms.\n",
        "Bucketizer class is handy to transform the data into various buckets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nobjRi4gSfu",
        "outputId": "dbc2586d-fb6e-46e6-f34a-1cf43a763774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from pyspark.ml.feature import  Bucketizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Define the splits for buckets\n",
        "splits = [-float(\"inf\"), -10, 0.0, 10, float(\"inf\")]\n",
        "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
        "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
        "b_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -800.0|\n",
            "|   -10.5|\n",
            "|    -1.7|\n",
            "|     0.0|\n",
            "|     8.2|\n",
            "|    90.1|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apnu5kOAg6l8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}